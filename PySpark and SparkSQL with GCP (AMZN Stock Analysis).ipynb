{"cells": [{"cell_type": "code", "execution_count": 1, "id": "8db0213e-1a2b-416c-aedf-18932b3503c7", "metadata": {}, "outputs": [], "source": "# 0: Start a Spark Session and import the libraries you will need\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "id": "289ccd65-9ace-4f6e-9d4b-990de6daf030", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+---------+---------+---------+---------+---------+--------+\n|      Date|     Open|     High|      Low|    Close|Adj Close|  Volume|\n+----------+---------+---------+---------+---------+---------+--------+\n|2013-01-02|12.804000|12.905000|12.663000|12.865500|12.865500|65420000|\n|2013-01-03|12.863500|13.044000|12.818500|12.924000|12.924000|55018000|\n|2013-01-04|12.879000|12.990000|12.832500|12.957500|12.957500|37484000|\n|2013-01-07|13.148500|13.486500|13.133500|13.423000|13.423000|98200000|\n|2013-01-08|13.353500|13.449000|13.178500|13.319000|13.319000|60214000|\n|2013-01-09|13.408500|13.475000|13.270000|13.317500|13.317500|45312000|\n|2013-01-10|13.427000|13.437000|13.115000|13.267000|13.267000|57268000|\n|2013-01-11|13.255000|13.421500|13.205500|13.397000|13.397000|48266000|\n|2013-01-14|13.400000|13.713000|13.377000|13.636500|13.636500|85500000|\n|2013-01-15|13.534000|13.636500|13.465000|13.595000|13.595000|46538000|\n+----------+---------+---------+---------+---------+---------+--------+\nonly showing top 10 rows\n\n"}], "source": "# Read in the data from the AMZN.csv file from the assignmnet folder\namzn_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"gs://lgenders-5132/assignment/AMZN.csv\")\namzn_df.show(10)"}, {"cell_type": "code", "execution_count": 3, "id": "843fcd3f-d2b0-4e9c-bb8b-4155d9033e12", "metadata": {}, "outputs": [], "source": "######################################################################################################"}, {"cell_type": "code", "execution_count": 4, "id": "7e672e59-affb-4758-b4a4-3c79c99ab8bf", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Date: string (nullable = true)\n |-- Open: string (nullable = true)\n |-- High: string (nullable = true)\n |-- Low: string (nullable = true)\n |-- Close: string (nullable = true)\n |-- Adj Close: string (nullable = true)\n |-- Volume: string (nullable = true)\n\n"}], "source": "# 1: Display the Schema of the data frame.\namzn_df.printSchema()"}, {"cell_type": "code", "execution_count": 5, "id": "74651a8c-a558-4a4e-8cdf-1b103c2adc83", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Date: date (nullable = true)\n |-- Open: double (nullable = true)\n |-- High: double (nullable = true)\n |-- Low: double (nullable = true)\n |-- Close: double (nullable = true)\n |-- Adj_Close: double (nullable = true)\n |-- Volume: integer (nullable = true)\n\n"}], "source": "# Need to change col types to correct data types\namzn_df_clean = amzn_df.withColumn(\"High\", amzn_df[\"High\"].cast(\"double\")).withColumn(\"Open\", amzn_df[\"Open\"].cast(\"double\")).withColumn(\"Low\", amzn_df[\"Low\"].cast(\"double\")).withColumn(\"Close\", amzn_df[\"Close\"].cast(\"double\"))\\\n                        .withColumn(\"Adj Close\", amzn_df[\"Adj Close\"].cast(\"double\")).withColumn(\"Volume\", amzn_df[\"Volume\"].cast(\"integer\")).withColumn(\"Date\", amzn_df[\"Date\"].cast(\"date\")).withColumnRenamed('Adj Close', 'Adj_Close')\namzn_df_clean.printSchema()"}, {"cell_type": "code", "execution_count": 6, "id": "d9dd27f7-61ae-41be-8929-080db203774a", "metadata": {}, "outputs": [], "source": "######################################################################################################"}, {"cell_type": "code", "execution_count": 7, "id": "9044ab2f-fbfe-45f2-9a02-9575dada2aeb", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "2518"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "#2: Display the summary statistics for all columns in the data frame.\n# Dropping duplicate values first\n# Show number of records before dropping\namzn_df_clean.count()"}, {"cell_type": "code", "execution_count": 8, "id": "358a11ea-8e7d-4606-8877-5bc2394f6f36", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "2518"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "# Drop duplicates and get new record count\namzn_df_clean_nodup = amzn_df.dropDuplicates()\namzn_df_clean_nodup.count()\n# No duplicate data"}, {"cell_type": "code", "execution_count": 9, "id": "1592362d-8a9a-4311-b1a6-3f991a4c0f67", "metadata": {}, "outputs": [{"data": {"text/plain": "2518"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "# Removing NAs\namzn_df_clean_noNAs = amzn_df.na.drop()\namzn_df_clean_noNAs.count()\n# no NAs"}, {"cell_type": "code", "execution_count": 10, "id": "5e1d7356-4dc4-4632-b3f4-d94b1bc736c9", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+-----------------+-----------------+-----------------+------------------+------------------+-------------------+\n|summary|             Open|             High|              Low|             Close|         Adj_Close|             Volume|\n+-------+-----------------+-----------------+-----------------+------------------+------------------+-------------------+\n|  count|             2518|             2518|             2518|              2518|              2518|               2518|\n|   mean|73.82353615726771|74.66189798570285| 72.8813564964257| 73.78004878752976| 73.78004878752976|8.025619868943606E7|\n| stddev|53.34565607175615|53.99876250199932|52.61413456378813|53.289557886169824|53.289557886169824|4.230300092993142E7|\n|    min|           12.447|          12.6465|          12.2875|           12.4115|           12.4115|           17626000|\n|    max|       187.199997|       188.654007|       184.839493|        186.570496|        186.570496|          477122000|\n+-------+-----------------+-----------------+-----------------+------------------+------------------+-------------------+\n\n"}], "source": "# Summary Statistics\namzn_df_clean.describe().show()"}, {"cell_type": "code", "execution_count": 11, "id": "7ab73749-caf1-4a2c-a37b-cd8a0756c3d5", "metadata": {}, "outputs": [], "source": "######################################################################################################"}, {"cell_type": "code", "execution_count": 12, "id": "88d51a86-4300-4f12-8907-314ec4e0ff95", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+----------+----------+----------+----------+----------+---------+\n|      Date|      Open|      High|       Low|     Close| Adj_Close|   Volume|\n+----------+----------+----------+----------+----------+----------+---------+\n|2021-07-13|185.104996|188.654007|183.565994|183.867996|183.867996| 76918000|\n|2021-11-19|185.634506|188.107498|183.785995|183.828506|183.828506| 98734000|\n|2021-07-08|182.177994|187.999496|   181.056|186.570496|186.570496|103612000|\n|2021-07-12|187.199997|187.864502|184.839493|185.927505|185.927505| 51432000|\n|2021-07-09|186.126007|187.399994|184.669998|185.966995|185.966995| 74964000|\n+----------+----------+----------+----------+----------+----------+---------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#3: Find and display the five records that has the highest price in the High column.\namzn_df_clean.createOrReplaceTempView('high_table')\n# Retrieve the top five records that have the highest price in the High col (currently as a string value)\namzn_df2_clean = spark.sql(\"SELECT * \\\n                    FROM high_table \\\n                    ORDER BY High DESC\").show(5)\n\n# Answer: The top 5 records with the highest price are 188.654007, 188.107498, 187.999496, 187.864502, and 187.399994\n# On the respective dates of 2021-07-13, 2021-11-19, 2021-07-08, 2021-07-12, and 2021-07-09"}, {"cell_type": "code", "execution_count": 13, "id": "20260b5a-4535-4bce-b5c4-c628187d5002", "metadata": {}, "outputs": [], "source": "######################################################################################################"}, {"cell_type": "code", "execution_count": 14, "id": "5631d2fc-44c3-4a4f-9c38-d02c1c628241", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-------+\n|      Date|    Low|\n+----------+-------+\n|2013-05-01|12.2875|\n+----------+-------+\nonly showing top 1 row\n\n"}], "source": "#4: What day had the lowest price? Display the date (as given in the Date column) and the price?\namzn_df_clean.createOrReplaceTempView('low_table')\n# Retrieve the lowest record and only show the date and price\namzn_df3_clean = spark.sql(\"SELECT Date, Low \\\n                            FROM low_table \\\n                            ORDER BY Low ASC\").show(1)\n\n# Answer: 2023-05-01 had the lowest price of 12.2875"}, {"cell_type": "code", "execution_count": 15, "id": "c5094d39-28de-4856-99ba-444da21e52ab", "metadata": {}, "outputs": [], "source": "######################################################################################################"}, {"cell_type": "code", "execution_count": 16, "id": "f90d56f0-feee-4d64-92b2-e221d5738f5b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+\n|Volume_range|\n+------------+\n|   459496000|\n+------------+\n\n"}], "source": "#5: What is the range of the Volume column? (Hint: Range is the difference between max and min values)\namzn_df_clean.createOrReplaceTempView('range_table')\n# Retrieve the difference between MAX(VOLUME) and MIN(VOLUME)\namzn_df4_clean = spark.sql(\"SELECT MAX(Volume) - MIN(Volume) as Volume_range \\\n                            FROM range_table\").show()\n# Answer: 459,496,000"}, {"cell_type": "code", "execution_count": 17, "id": "7b4d372a-c86d-461b-ba12-ce31fc930d09", "metadata": {}, "outputs": [], "source": "######################################################################################################"}, {"cell_type": "code", "execution_count": 18, "id": "ca174af1-5aed-4acc-9c16-f59db509644e", "metadata": {}, "outputs": [{"data": {"text/plain": "74.66189798570285"}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": "#6: Calculate the average of the High column. What percent of the observations had a High price greater than the average?\namzn_df_clean.createOrReplaceTempView('avg_table')\n# Retrive the average of the High column first\nhigh_avg = spark.sql(\"SELECT AVG(High) \\\n                            FROM avg_table\").collect()[0][0] # returns the value 74.66189798570285 by using indices\n# show the average of the High column\nhigh_avg\n# Answer: 74.66189798570285"}, {"cell_type": "code", "execution_count": 19, "id": "ffb705b5-d8e7-485c-be89-c9c5c172e9d1", "metadata": {}, "outputs": [{"data": {"text/plain": "1208"}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": "# What percent of the observations had a High price greater than the average?\nobs_greater_avg = amzn_df_clean.filter(amzn_df_clean['High'] > high_avg).count()\nobs_greater_avg # 1208 obs greater than the average, now need total observation count"}, {"cell_type": "code", "execution_count": 20, "id": "35cad094-ceee-4b2f-8a30-9f8f811ff6ef", "metadata": {}, "outputs": [{"data": {"text/plain": "2518"}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": "# total obs\ntotal_count = amzn_df_clean.count()\ntotal_count # 2518 (matches summary stats from above)"}, {"cell_type": "code", "execution_count": 21, "id": "c0032b20-4cf8-4f6e-b568-90e04c04306e", "metadata": {}, "outputs": [{"data": {"text/plain": "47.97458300238284"}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}], "source": "# calculate percent of obs had a High price greater than the average\nprcnt_above_avg = (obs_greater_avg/total_count)*100\nprcnt_above_avg\n# Answer: 47.97458300238284 % of the observations had a High price greater than the average"}, {"cell_type": "code", "execution_count": 22, "id": "7660f431-5921-4379-bb31-550ed70ae8d2", "metadata": {}, "outputs": [], "source": "######################################################################################################"}, {"cell_type": "code", "execution_count": 23, "id": "8b8da2a2-36c6-45d3-bb50-ab8082c7ad79", "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[Date: date, Open: double, High: double, Low: double, Close: double, Adj_Close: double, Volume: int, year: int]"}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": "#7: What is the total Volume per year? Sort the results based on the total Volume in a decreasing order.\n# Make new col of year data\n# import sql.functions\nfrom pyspark.sql.functions import year\namzn_df6_clean = amzn_df_clean.withColumn('year', year('Date'))\namzn_df6_clean"}, {"cell_type": "code", "execution_count": 24, "id": "5dc4a0bf-fba4-4df4-82a1-cece429a1264", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 31:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----+-----------+\n|year|sum(volume)|\n+----+-----------+\n|2018|28357952000|\n|2020|24950814000|\n|2016|20775126000|\n|2014|20581334000|\n|2019|19493002000|\n|2015|19142040000|\n|2022|19096256300|\n|2017|17654108000|\n|2021|17076362000|\n|2013|14958114000|\n+----+-----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Group by year, sort in descending order, show it\ntotal_vol = amzn_df6_clean.groupBy('year').sum('volume').sort('sum(volume)', ascending = False).show()"}, {"cell_type": "code", "execution_count": 25, "id": "8d41eb2b-a4f1-471d-989f-406007094f78", "metadata": {}, "outputs": [], "source": "######################################################################################################"}, {"cell_type": "code", "execution_count": 26, "id": "44d7ef46-b5a8-4e35-9597-a515b0b46f82", "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[Date: date, Open: double, High: double, Low: double, Close: double, Adj_Close: double, Volume: int, month: int]"}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": "#8: What is the average Adjusted Close price for each month? Sort the results in an increasing order based on the average Adjusted Close price\n# Make new col of month data\nfrom pyspark.sql.functions import month\namzn_df7_clean = amzn_df_clean.withColumn('month', month('Date'))\namzn_df7_clean"}, {"cell_type": "code", "execution_count": 27, "id": "9c121242-b4e9-4cbb-870a-ad128d211ed2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-----------------+\n|month|    adj_close_avg|\n+-----+-----------------+\n|    1|67.00627561083743|\n|    2|68.95255491623037|\n|    5|69.37138398578199|\n|    3|69.88013780275226|\n|    4|71.78249272596155|\n|    6|73.02435191079809|\n|   12|75.53066592417065|\n|   10|76.20908837556561|\n|   11|76.53358077450977|\n|    7|77.88898079245286|\n|    9|78.86672661951219|\n|    8| 79.5366561040724|\n+-----+-----------------+\n\n"}], "source": "amzn_df7_clean.createOrReplaceTempView('avg_monthly_table')\n# Retrive the average of the adj close price column first\navg_mon_sorted = spark.sql(\"SELECT month, AVG(Adj_Close) as adj_close_avg FROM avg_monthly_table GROUP BY month ORDER BY adj_close_avg ASC\").show()\navg_mon_sorted"}, {"cell_type": "code", "execution_count": 28, "id": "fa459a3b-b4a3-457f-93b1-0d53997ba2a6", "metadata": {}, "outputs": [], "source": "######################################################################################################"}, {"cell_type": "code", "execution_count": 29, "id": "a4963bc2-24c0-42b8-b647-a565ededff80", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-------+-------+---------------+\n|      Date|  Close|   Open|open_close_diff|\n+----------+-------+-------+---------------+\n|2013-01-08| 13.319|13.3535|           0.03|\n|2013-01-09|13.3175|13.4085|           0.09|\n|2013-01-10| 13.267| 13.427|           0.16|\n|2013-01-16|13.4465|13.5265|           0.08|\n|2013-01-17| 13.524| 13.575|           0.05|\n|2013-01-22|13.5095| 13.581|           0.07|\n|2013-01-23|13.4055|13.5285|           0.12|\n|2013-01-28| 13.802| 14.189|           0.39|\n|2013-01-29|13.0175|13.7675|           0.75|\n|2013-01-30| 13.638|  14.15|           0.51|\n|2013-01-31| 13.275| 13.552|           0.28|\n|2013-02-01|  13.25|13.4465|            0.2|\n|2013-02-04| 12.999| 13.139|           0.14|\n|2013-02-06| 13.111| 13.258|           0.15|\n|2013-02-07|13.0115| 13.205|           0.19|\n|2013-02-11|12.8605|  13.16|            0.3|\n|2013-02-12| 12.935|12.9595|           0.02|\n|2013-02-15|13.2545|13.3815|           0.13|\n|2013-02-20|13.3205|  13.51|           0.19|\n|2013-02-22| 13.271| 13.331|           0.06|\n+----------+-------+-------+---------------+\nonly showing top 20 rows\n\n"}], "source": "#9: Using SQL commands, create a new data frame that includes the records for the days where \n# closing price is lower than the opening price. Display the number of records in this new data frame.\namzn_df_clean.createOrReplaceTempView('close_lwr_open')\n# Retrieve the df where close < open first\n# Creating a col called open_close_diff to check work\nclose_lessthan = spark.sql(\"SELECT Date, Close, Open, ROUND((Open - Close),2) AS open_close_diff FROM close_lwr_open WHERE Close < Open\").show()"}, {"cell_type": "code", "execution_count": 30, "id": "f72fa913-90f6-4215-b611-428714cabfc5", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+\n|count(1)|\n+--------+\n|    1272|\n+--------+\n\n"}], "source": "# Display the number of records in this new data frame (close_lessthan) - continuing to use SQL\namzn_df_clean.createOrReplaceTempView('close_lwr_open')\n# Retrieve the number of records in close_lessthan\nnum_records = spark.sql(\"SELECT COUNT(*) FROM close_lwr_open WHERE Close < Open\").show()\n# Answer: 1272 records where close is less than open price"}, {"cell_type": "code", "execution_count": 31, "id": "633067af-94b2-4748-8a9a-ec824011b69b", "metadata": {}, "outputs": [], "source": "######################################################################################################"}, {"cell_type": "code", "execution_count": 32, "id": "21ac4e7e-95aa-4b47-b560-2ab2c3e2660d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-------+-------+-------+-------+---------+---------+----+-----+\n|      Date|   Open|   High|    Low|  Close|Adj_Close|   Volume|Year|Month|\n+----------+-------+-------+-------+-------+---------+---------+----+-----+\n|2013-01-02| 12.804| 12.905| 12.663|12.8655|  12.8655| 65420000|2013|    1|\n|2013-01-03|12.8635| 13.044|12.8185| 12.924|   12.924| 55018000|2013|    1|\n|2013-01-04| 12.879|  12.99|12.8325|12.9575|  12.9575| 37484000|2013|    1|\n|2013-01-07|13.1485|13.4865|13.1335| 13.423|   13.423| 98200000|2013|    1|\n|2013-01-08|13.3535| 13.449|13.1785| 13.319|   13.319| 60214000|2013|    1|\n|2013-01-09|13.4085| 13.475|  13.27|13.3175|  13.3175| 45312000|2013|    1|\n|2013-01-10| 13.427| 13.437| 13.115| 13.267|   13.267| 57268000|2013|    1|\n|2013-01-11| 13.255|13.4215|13.2055| 13.397|   13.397| 48266000|2013|    1|\n|2013-01-14|   13.4| 13.713| 13.377|13.6365|  13.6365| 85500000|2013|    1|\n|2013-01-15| 13.534|13.6365| 13.465| 13.595|   13.595| 46538000|2013|    1|\n|2013-01-16|13.5265| 13.562|13.3915|13.4465|  13.4465| 41312000|2013|    1|\n|2013-01-17| 13.575|13.5985|13.4605| 13.524|   13.524| 37692000|2013|    1|\n|2013-01-18|13.5415| 13.725|  13.48| 13.606|   13.606| 58840000|2013|    1|\n|2013-01-22| 13.581| 13.605|13.4615|13.5095|  13.5095| 42754000|2013|    1|\n|2013-01-23|13.5285|13.5545|13.3325|13.4055|  13.4055| 50178000|2013|    1|\n|2013-01-24|13.4685|13.8325|13.4685| 13.673|   13.673| 68340000|2013|    1|\n|2013-01-25|  13.75| 14.236|  13.72|14.1995|  14.1995| 99362000|2013|    1|\n|2013-01-28| 14.189| 14.224|  13.72| 13.802|   13.802| 86428000|2013|    1|\n|2013-01-29|13.7675| 13.773|12.9175|13.0175|  13.0175|203452000|2013|    1|\n|2013-01-30|  14.15|  14.21|13.3555| 13.638|   13.638|261508000|2013|    1|\n+----------+-------+-------+-------+-------+---------+---------+----+-----+\nonly showing top 20 rows\n\n"}], "source": "#10: Perform the following using SQL commands: Count the number of days in each month of each\n# year during the last three years, where closing price is at least $5 less than the opening price. The\n# resulting data frame should be sorted based on Year first and then Month in increasing order.\n# Make new col of month data and year data\nfrom pyspark.sql.functions import month\nfrom pyspark.sql.functions import year\namzn_df10_clean = amzn_df_clean.withColumn('Year', year('Date')).withColumn('Month', month('Date'))\namzn_df10_clean.show()"}, {"cell_type": "code", "execution_count": 33, "id": "b1c121d2-f806-4f34-b791-6c3eba45d380", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+----+-----+----------+----------+---------------+\n|      Date|Year|Month|      Open|     Close|Open_Close_Diff|\n+----------+----+-----+----------+----------+---------------+\n|2018-10-29|2018|   10|      83.0|    76.944|            6.1|\n|2018-10-24|2018|   10| 88.684998| 83.209999|            5.5|\n|2018-10-10|2018|   10| 92.894501| 87.762497|            5.1|\n|2020-07-13|2020|    7|162.552994|155.199997|            7.4|\n|2020-07-23|2020|    7|154.913498|149.327499|            5.6|\n|2020-09-16|2020|    9|158.999496|153.904999|            5.1|\n|2020-09-10|2020|    9|165.360992|158.755493|            6.6|\n|2020-09-23|2020|    9|  156.0215|149.992996|            6.0|\n|2020-09-03|2020|    9|    174.25|168.399994|            5.9|\n|2020-10-30|2020|   10|157.887497|151.807495|            6.1|\n|2021-01-27|2021|    1|167.074493|161.628998|            5.4|\n|2021-02-03|2021|    2|171.250504|165.626495|            5.6|\n|2021-11-22|2021|   11|   183.819|178.628494|            5.2|\n|2021-12-01|2021|   12|    177.25|172.186005|            5.1|\n|2022-01-20|2022|    1|156.766006|151.667496|            5.1|\n|2022-01-21|2022|    1|149.949997|142.643005|            7.3|\n|2022-01-26|2022|    1|    144.75|138.872498|            5.9|\n|2022-02-23|2022|    2|151.650497|144.826996|            6.8|\n|2022-03-03|2022|    3|153.531494|147.898499|            5.6|\n|2022-03-07|2022|    3|145.443497|137.453003|            8.0|\n+----------+----+-----+----------+----------+---------------+\nonly showing top 20 rows\n\n"}], "source": "amzn_df10_clean.createOrReplaceTempView('question_10')\n# Retrieve the dates where closing price is at least $5 less than the opening price and sort by year then month in increasing order\n# Adding a col that takes the open-close difference to check work and naming Open_Close_Diff\nq_10 = spark.sql(\"SELECT Date, Year, Month, Open, Close, ROUND((Open - Close), 1) as Open_Close_Diff FROM question_10 WHERE Open - Close >= 5 ORDER BY Year ASC, Month ASC\").show()"}, {"cell_type": "code", "execution_count": 34, "id": "2317e402-9354-472a-a287-bb0922de764b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+-----+--------+\n|Year|Month|count(1)|\n+----+-----+--------+\n|2022|   10|       1|\n|2018|   10|       3|\n|2020|    9|       4|\n|2022|    2|       1|\n|2021|   11|       1|\n|2022|   11|       2|\n|2022|    3|       2|\n|2021|   12|       1|\n|2021|    2|       1|\n|2022|    1|       3|\n|2022|    5|       1|\n|2021|    1|       1|\n|2020|    7|       2|\n|2020|   10|       1|\n|2022|    6|       1|\n|2022|    4|       3|\n|2022|    8|       1|\n+----+-----+--------+\n\n"}], "source": "# Retrieve the number of records in q_10 by each year and each month\namzn_df10_clean.createOrReplaceTempView('question_10')\nnum_records_10 = spark.sql(\"SELECT Year, Month, COUNT(*) FROM question_10 WHERE Open - Close >= 5 GROUP BY Year, Month\").show()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}